{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "U0KsVUlH6REi"
   },
   "source": [
    "# Advanced Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6pCKRUFS6REj"
   },
   "source": [
    "In the Advanced Modeling phase, we'll look at different techniques that can be used to iteratively improve the baseline model. As said earlier, often, baseline models deliver 90% of value for 10% of the effort, so a lot of the improvements we'll be investigating will yield little to no improvement on the final model.\n",
    "\n",
    "There are few methods that can be used for improving (an already good) baseline model:\n",
    "- Feature analysis & selection. \n",
    "- Model selection.\n",
    "- Hyperparameter tuning.\n",
    "\n",
    "Alternatively, we could also look at techniques such as 'outlier removal' but these are unlikely to yield significant improvements as the data is already cleaned up."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yKVG05MJ6REj"
   },
   "source": [
    "## Data Loading\n",
    "Import the required libraries to build a simple baseline model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "gLzewn6_6REk"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'boruta'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-10fea3b17ca0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mboruta\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mBorutaPy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'matplotlib'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'inline'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minsert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'../src/'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'boruta'"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import math\n",
    "import json\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from boruta import BorutaPy\n",
    "%matplotlib inline\n",
    "sys.path.insert(1, '../src/')\n",
    "from utils import load_dataset_data\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.model_selection import LeaveOneGroupOut, GroupKFold, cross_validate, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import plot_confusion_matrix, f1_score, confusion_matrix, accuracy_score, classification_report, roc_curve, roc_auc_score,auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "XTqu7F9e6REn"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'load_dataset_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-2a16672b8dd9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msubject_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msubject_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_dataset_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'load_dataset_data' is not defined"
     ]
    }
   ],
   "source": [
    "X_train, y_train, subject_train, X_test, y_test, subject_test = load_dataset_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7PWqTuOn6REq"
   },
   "source": [
    "## Feature analysis & selection\n",
    "Feature analysis & selection, is the process of selecting a subset of relevant features for use in model development. Feature selection techniques are used for several reasons:\n",
    "- they allow for simplification of models to make them easier to interpret\n",
    "- they allow for shorter training times (smaller datasets require less training time)\n",
    "- they avoid the curse of dimensionality & resultin  enhanced generalization by reducing overfitting\n",
    "\n",
    "Note that we'll only be using the training data to perform feature selection, as we don't want to overfit any of the techniques to improve the model onto the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kzdzZUrx6REr",
    "outputId": "b506e87e-e708-48f0-8373-5921f2e098ea"
   },
   "outputs": [],
   "source": [
    "corr_matrix = X_train.corr()\n",
    "\n",
    "f = plt.figure(figsize=(19, 15))\n",
    "plt.matshow(corr_matrix, fignum=f.number)\n",
    "cb = plt.colorbar()\n",
    "cb.ax.tick_params(labelsize=14)\n",
    "plt.title('Correlation Matrix', fontsize=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s4UNYcki6REu"
   },
   "source": [
    "The large clusters of correlated features indicates that some of the features are very similar. Within each cluster, it probably suffices to select 1 feature for the model, as the other features will likely introduce unnecessary noise.\n",
    "\n",
    "We'll investigate two approaches to select the relevant features:\n",
    "- a filter method, 'Mutual Information'. In filter methods, features are selected on the basis of their scores in various statistical tests for their correlation with the outcome variable. In the 'Mutual Information' test, we compute the correlation between the categorical target and features, which is a proxy for how well the features predict the target.\n",
    "- a wrapper method, 'Boruta'. In wrapper methods, we evaluate a subset of features using a machine learning algorithm that employs a search strategy to look through the space of possible feature subsets, evaluating each subset based on the quality of the performance of the given algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "p_4zdFuD6REu",
    "outputId": "461c3a23-a6c8-4061-ca7d-5cc3cb466311"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mutual_info_classif' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-3919e3636407>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmi\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmutual_info_classif\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mmi_feature_importance\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mmi_feature_importance\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msort\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m-\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'mutual_info_classif' is not defined"
     ]
    }
   ],
   "source": [
    "mi = mutual_info_classif(X_train, y_train.values.ravel())\n",
    "\n",
    "mi_feature_importance = list(zip(X_train.columns, mi))\n",
    "mi_feature_importance.sort(key = lambda x: -x[1])\n",
    "\n",
    "mi_unimportant_features = [feature for (feature, mi) in mi_feature_importance if mi<0.1]\n",
    "\n",
    "# plot mi feature importance\n",
    "plt.bar([x for x in range(len(mi_feature_importance))], [x[1] for x in (mi_feature_importance)])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dfd8fGtU6REw"
   },
   "source": [
    "Clearly, some of the features are less important than others. For now, we'll filter out any features that drop below a Mutual Importance of 0.1. Let's investigate whether Boruta finds additional features that should be removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y2a6cq596REx",
    "outputId": "47b4517f-7447-4137-f9c2-b493b9b0f3ea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: \t1 / 100\n",
      "Confirmed: \t0\n",
      "Tentative: \t561\n",
      "Rejected: \t0\n",
      "Iteration: \t2 / 100\n",
      "Confirmed: \t0\n",
      "Tentative: \t561\n",
      "Rejected: \t0\n",
      "Iteration: \t3 / 100\n",
      "Confirmed: \t0\n",
      "Tentative: \t561\n",
      "Rejected: \t0\n",
      "Iteration: \t4 / 100\n",
      "Confirmed: \t0\n",
      "Tentative: \t561\n",
      "Rejected: \t0\n",
      "Iteration: \t5 / 100\n",
      "Confirmed: \t0\n",
      "Tentative: \t561\n",
      "Rejected: \t0\n",
      "Iteration: \t6 / 100\n",
      "Confirmed: \t0\n",
      "Tentative: \t561\n",
      "Rejected: \t0\n",
      "Iteration: \t7 / 100\n",
      "Confirmed: \t0\n",
      "Tentative: \t561\n",
      "Rejected: \t0\n",
      "Iteration: \t8 / 100\n",
      "Confirmed: \t269\n",
      "Tentative: \t186\n",
      "Rejected: \t106\n",
      "Iteration: \t9 / 100\n",
      "Confirmed: \t269\n",
      "Tentative: \t186\n",
      "Rejected: \t106\n",
      "Iteration: \t10 / 100\n",
      "Confirmed: \t269\n",
      "Tentative: \t186\n",
      "Rejected: \t106\n",
      "Iteration: \t11 / 100\n",
      "Confirmed: \t269\n",
      "Tentative: \t186\n",
      "Rejected: \t106\n",
      "Iteration: \t12 / 100\n",
      "Confirmed: \t282\n",
      "Tentative: \t173\n",
      "Rejected: \t106\n",
      "Iteration: \t13 / 100\n",
      "Confirmed: \t282\n",
      "Tentative: \t156\n",
      "Rejected: \t123\n",
      "Iteration: \t14 / 100\n",
      "Confirmed: \t282\n",
      "Tentative: \t156\n",
      "Rejected: \t123\n",
      "Iteration: \t15 / 100\n",
      "Confirmed: \t282\n",
      "Tentative: \t156\n",
      "Rejected: \t123\n",
      "Iteration: \t16 / 100\n",
      "Confirmed: \t292\n",
      "Tentative: \t146\n",
      "Rejected: \t123\n",
      "Iteration: \t17 / 100\n",
      "Confirmed: \t292\n",
      "Tentative: \t146\n",
      "Rejected: \t123\n",
      "Iteration: \t18 / 100\n",
      "Confirmed: \t292\n",
      "Tentative: \t138\n",
      "Rejected: \t131\n",
      "Iteration: \t19 / 100\n",
      "Confirmed: \t297\n",
      "Tentative: \t133\n",
      "Rejected: \t131\n",
      "Iteration: \t20 / 100\n",
      "Confirmed: \t297\n",
      "Tentative: \t133\n",
      "Rejected: \t131\n",
      "Iteration: \t21 / 100\n",
      "Confirmed: \t297\n",
      "Tentative: \t126\n",
      "Rejected: \t138\n",
      "Iteration: \t22 / 100\n",
      "Confirmed: \t301\n",
      "Tentative: \t122\n",
      "Rejected: \t138\n",
      "Iteration: \t23 / 100\n",
      "Confirmed: \t301\n",
      "Tentative: \t122\n",
      "Rejected: \t138\n",
      "Iteration: \t24 / 100\n",
      "Confirmed: \t301\n",
      "Tentative: \t113\n",
      "Rejected: \t147\n",
      "Iteration: \t25 / 100\n",
      "Confirmed: \t301\n",
      "Tentative: \t113\n",
      "Rejected: \t147\n",
      "Iteration: \t26 / 100\n",
      "Confirmed: \t302\n",
      "Tentative: \t112\n",
      "Rejected: \t147\n",
      "Iteration: \t27 / 100\n",
      "Confirmed: \t302\n",
      "Tentative: \t112\n",
      "Rejected: \t147\n",
      "Iteration: \t28 / 100\n",
      "Confirmed: \t302\n",
      "Tentative: \t112\n",
      "Rejected: \t147\n",
      "Iteration: \t29 / 100\n",
      "Confirmed: \t302\n",
      "Tentative: \t102\n",
      "Rejected: \t157\n",
      "Iteration: \t30 / 100\n",
      "Confirmed: \t302\n",
      "Tentative: \t102\n",
      "Rejected: \t157\n",
      "Iteration: \t31 / 100\n",
      "Confirmed: \t302\n",
      "Tentative: \t102\n",
      "Rejected: \t157\n",
      "Iteration: \t32 / 100\n",
      "Confirmed: \t303\n",
      "Tentative: \t101\n",
      "Rejected: \t157\n",
      "Iteration: \t33 / 100\n",
      "Confirmed: \t303\n",
      "Tentative: \t101\n",
      "Rejected: \t157\n",
      "Iteration: \t34 / 100\n",
      "Confirmed: \t303\n",
      "Tentative: \t97\n",
      "Rejected: \t161\n",
      "Iteration: \t35 / 100\n",
      "Confirmed: \t303\n",
      "Tentative: \t97\n",
      "Rejected: \t161\n",
      "Iteration: \t36 / 100\n",
      "Confirmed: \t303\n",
      "Tentative: \t92\n",
      "Rejected: \t166\n",
      "Iteration: \t37 / 100\n",
      "Confirmed: \t304\n",
      "Tentative: \t91\n",
      "Rejected: \t166\n",
      "Iteration: \t38 / 100\n",
      "Confirmed: \t304\n",
      "Tentative: \t91\n",
      "Rejected: \t166\n",
      "Iteration: \t39 / 100\n",
      "Confirmed: \t304\n",
      "Tentative: \t91\n",
      "Rejected: \t166\n",
      "Iteration: \t40 / 100\n",
      "Confirmed: \t304\n",
      "Tentative: \t91\n",
      "Rejected: \t166\n",
      "Iteration: \t41 / 100\n",
      "Confirmed: \t304\n",
      "Tentative: \t89\n",
      "Rejected: \t168\n",
      "Iteration: \t42 / 100\n",
      "Confirmed: \t304\n",
      "Tentative: \t89\n",
      "Rejected: \t168\n",
      "Iteration: \t43 / 100\n",
      "Confirmed: \t305\n",
      "Tentative: \t85\n",
      "Rejected: \t171\n",
      "Iteration: \t44 / 100\n",
      "Confirmed: \t305\n",
      "Tentative: \t85\n",
      "Rejected: \t171\n",
      "Iteration: \t45 / 100\n",
      "Confirmed: \t305\n",
      "Tentative: \t85\n",
      "Rejected: \t171\n",
      "Iteration: \t46 / 100\n",
      "Confirmed: \t305\n",
      "Tentative: \t85\n",
      "Rejected: \t171\n",
      "Iteration: \t47 / 100\n",
      "Confirmed: \t305\n",
      "Tentative: \t85\n",
      "Rejected: \t171\n",
      "Iteration: \t48 / 100\n",
      "Confirmed: \t305\n",
      "Tentative: \t84\n",
      "Rejected: \t172\n",
      "Iteration: \t49 / 100\n",
      "Confirmed: \t306\n",
      "Tentative: \t83\n",
      "Rejected: \t172\n",
      "Iteration: \t50 / 100\n",
      "Confirmed: \t306\n",
      "Tentative: \t83\n",
      "Rejected: \t172\n",
      "Iteration: \t51 / 100\n",
      "Confirmed: \t308\n",
      "Tentative: \t81\n",
      "Rejected: \t172\n",
      "Iteration: \t52 / 100\n",
      "Confirmed: \t308\n",
      "Tentative: \t76\n",
      "Rejected: \t177\n",
      "Iteration: \t53 / 100\n",
      "Confirmed: \t308\n",
      "Tentative: \t76\n",
      "Rejected: \t177\n",
      "Iteration: \t54 / 100\n",
      "Confirmed: \t310\n",
      "Tentative: \t74\n",
      "Rejected: \t177\n",
      "Iteration: \t55 / 100\n",
      "Confirmed: \t310\n",
      "Tentative: \t74\n",
      "Rejected: \t177\n",
      "Iteration: \t56 / 100\n",
      "Confirmed: \t310\n",
      "Tentative: \t74\n",
      "Rejected: \t177\n",
      "Iteration: \t57 / 100\n",
      "Confirmed: \t310\n",
      "Tentative: \t74\n",
      "Rejected: \t177\n",
      "Iteration: \t58 / 100\n",
      "Confirmed: \t310\n",
      "Tentative: \t74\n",
      "Rejected: \t177\n",
      "Iteration: \t59 / 100\n",
      "Confirmed: \t310\n",
      "Tentative: \t74\n",
      "Rejected: \t177\n",
      "Iteration: \t60 / 100\n",
      "Confirmed: \t310\n",
      "Tentative: \t74\n",
      "Rejected: \t177\n",
      "Iteration: \t61 / 100\n",
      "Confirmed: \t310\n",
      "Tentative: \t71\n",
      "Rejected: \t180\n",
      "Iteration: \t62 / 100\n",
      "Confirmed: \t310\n",
      "Tentative: \t71\n",
      "Rejected: \t180\n",
      "Iteration: \t63 / 100\n",
      "Confirmed: \t310\n",
      "Tentative: \t71\n",
      "Rejected: \t180\n",
      "Iteration: \t64 / 100\n",
      "Confirmed: \t310\n",
      "Tentative: \t71\n",
      "Rejected: \t180\n",
      "Iteration: \t65 / 100\n",
      "Confirmed: \t310\n",
      "Tentative: \t71\n",
      "Rejected: \t180\n",
      "Iteration: \t66 / 100\n",
      "Confirmed: \t310\n",
      "Tentative: \t71\n",
      "Rejected: \t180\n",
      "Iteration: \t67 / 100\n",
      "Confirmed: \t310\n",
      "Tentative: \t71\n",
      "Rejected: \t180\n",
      "Iteration: \t68 / 100\n",
      "Confirmed: \t310\n",
      "Tentative: \t71\n",
      "Rejected: \t180\n",
      "Iteration: \t69 / 100\n",
      "Confirmed: \t310\n",
      "Tentative: \t68\n",
      "Rejected: \t183\n",
      "Iteration: \t70 / 100\n",
      "Confirmed: \t310\n",
      "Tentative: \t68\n",
      "Rejected: \t183\n",
      "Iteration: \t71 / 100\n",
      "Confirmed: \t310\n",
      "Tentative: \t68\n",
      "Rejected: \t183\n",
      "Iteration: \t72 / 100\n",
      "Confirmed: \t310\n",
      "Tentative: \t68\n",
      "Rejected: \t183\n",
      "Iteration: \t73 / 100\n",
      "Confirmed: \t310\n",
      "Tentative: \t68\n",
      "Rejected: \t183\n",
      "Iteration: \t74 / 100\n",
      "Confirmed: \t310\n",
      "Tentative: \t68\n",
      "Rejected: \t183\n",
      "Iteration: \t75 / 100\n",
      "Confirmed: \t310\n",
      "Tentative: \t66\n",
      "Rejected: \t185\n",
      "Iteration: \t76 / 100\n",
      "Confirmed: \t310\n",
      "Tentative: \t66\n",
      "Rejected: \t185\n",
      "Iteration: \t77 / 100\n",
      "Confirmed: \t310\n",
      "Tentative: \t66\n",
      "Rejected: \t185\n",
      "Iteration: \t78 / 100\n",
      "Confirmed: \t310\n",
      "Tentative: \t66\n",
      "Rejected: \t185\n",
      "Iteration: \t79 / 100\n",
      "Confirmed: \t310\n",
      "Tentative: \t66\n",
      "Rejected: \t185\n",
      "Iteration: \t80 / 100\n",
      "Confirmed: \t310\n",
      "Tentative: \t66\n",
      "Rejected: \t185\n",
      "Iteration: \t81 / 100\n",
      "Confirmed: \t310\n",
      "Tentative: \t66\n",
      "Rejected: \t185\n",
      "Iteration: \t82 / 100\n",
      "Confirmed: \t310\n",
      "Tentative: \t66\n",
      "Rejected: \t185\n",
      "Iteration: \t83 / 100\n",
      "Confirmed: \t310\n",
      "Tentative: \t64\n",
      "Rejected: \t187\n",
      "Iteration: \t84 / 100\n",
      "Confirmed: \t310\n",
      "Tentative: \t64\n",
      "Rejected: \t187\n",
      "Iteration: \t85 / 100\n",
      "Confirmed: \t311\n",
      "Tentative: \t63\n",
      "Rejected: \t187\n",
      "Iteration: \t86 / 100\n",
      "Confirmed: \t311\n",
      "Tentative: \t63\n",
      "Rejected: \t187\n",
      "Iteration: \t87 / 100\n",
      "Confirmed: \t311\n",
      "Tentative: \t63\n",
      "Rejected: \t187\n",
      "Iteration: \t88 / 100\n",
      "Confirmed: \t311\n",
      "Tentative: \t63\n",
      "Rejected: \t187\n",
      "Iteration: \t89 / 100\n",
      "Confirmed: \t311\n",
      "Tentative: \t62\n",
      "Rejected: \t188\n",
      "Iteration: \t90 / 100\n",
      "Confirmed: \t311\n",
      "Tentative: \t62\n",
      "Rejected: \t188\n",
      "Iteration: \t91 / 100\n",
      "Confirmed: \t311\n",
      "Tentative: \t62\n",
      "Rejected: \t188\n",
      "Iteration: \t92 / 100\n",
      "Confirmed: \t311\n",
      "Tentative: \t62\n",
      "Rejected: \t188\n",
      "Iteration: \t93 / 100\n",
      "Confirmed: \t311\n",
      "Tentative: \t62\n",
      "Rejected: \t188\n",
      "Iteration: \t94 / 100\n",
      "Confirmed: \t311\n",
      "Tentative: \t61\n",
      "Rejected: \t189\n",
      "Iteration: \t95 / 100\n",
      "Confirmed: \t311\n",
      "Tentative: \t61\n",
      "Rejected: \t189\n",
      "Iteration: \t96 / 100\n",
      "Confirmed: \t311\n",
      "Tentative: \t61\n",
      "Rejected: \t189\n",
      "Iteration: \t97 / 100\n",
      "Confirmed: \t311\n",
      "Tentative: \t61\n",
      "Rejected: \t189\n",
      "Iteration: \t98 / 100\n",
      "Confirmed: \t311\n",
      "Tentative: \t61\n",
      "Rejected: \t189\n",
      "Iteration: \t99 / 100\n",
      "Confirmed: \t311\n",
      "Tentative: \t60\n",
      "Rejected: \t190\n",
      "\n",
      "\n",
      "BorutaPy finished running.\n",
      "\n",
      "Iteration: \t100 / 100\n",
      "Confirmed: \t311\n",
      "Tentative: \t32\n",
      "Rejected: \t190\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BorutaPy(alpha=0.05,\n",
       "         estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,\n",
       "                                          class_weight='balanced',\n",
       "                                          criterion='gini', max_depth=5,\n",
       "                                          max_features='auto',\n",
       "                                          max_leaf_nodes=None, max_samples=None,\n",
       "                                          min_impurity_decrease=0.0,\n",
       "                                          min_impurity_split=None,\n",
       "                                          min_samples_leaf=1,\n",
       "                                          min_samples_split=2,\n",
       "                                          min_weight_fraction_leaf=0.0,\n",
       "                                          n_estimators=545, n_jobs=4,\n",
       "                                          oob_score=False,\n",
       "                                          random_state=RandomState(MT19937) at 0x7FECF6768140,\n",
       "                                          verbose=0, warm_start=False),\n",
       "         max_iter=100, n_estimators='auto', perc=100,\n",
       "         random_state=RandomState(MT19937) at 0x7FECF6768140, two_step=True,\n",
       "         verbose=2)"
      ]
     },
     "execution_count": 5,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define a random forest classifier to evaluate whether a feature subset is good.\n",
    "rf = RandomForestClassifier(n_jobs=4, class_weight='balanced', max_depth=5)\n",
    "\n",
    "# Define Boruta feature selection method\n",
    "boruta_feat_selector = BorutaPy(rf, n_estimators='auto', verbose=2, random_state=1)\n",
    "\n",
    "# Find all relevant features\n",
    "boruta_feat_selector.fit(X_train.values, y_train.values.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "mQO3g6Ap6REz"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-18639df26dd6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mboruta_unimportant_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m~\u001b[0m\u001b[0mboruta_feat_selector\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msupport_\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "boruta_unimportant_features = list(X_train.columns[~boruta_feat_selector.support_])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BgIc9r0l6RE7"
   },
   "outputs": [],
   "source": [
    "unimportant_features = {'boruta': boruta_unimportant_features, 'mi_unimportant_features': mi_unimportant_features}\n",
    "with open('unimportant_features.json', 'w') as json_file:\n",
    "    json.dump(unimportant_features, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4guvGB8x6RE9"
   },
   "outputs": [],
   "source": [
    "with open('unimportant_features.json', 'r') as json_file:\n",
    "    unimportant_features = json.load(json_file)\n",
    "boruta_unimportant_features = unimportant_features['boruta']\n",
    "mi_unimportant_features = unimportant_features['mi_unimportant_features']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yIDhTLPE6RE_",
    "outputId": "1b5655b0-2d08-41c6-c482-4bde157f769e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(44, 46, 250)"
      ]
     },
     "execution_count": 9,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(boruta_unimportant_features)&set(mi_unimportant_features)), len(mi_unimportant_features), len(boruta_unimportant_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PQTG7ZrC6RFB"
   },
   "source": [
    "Boruta finds a larger set of unimportant features than mutual information. For the next experiment, we'll use the smallest set of informative features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ex2geOBS6RFB"
   },
   "source": [
    "##  Model selection\n",
    "\n",
    "Model selection is the task of selecting a statistical model from a list of candidate models. \n",
    "Given a range of candidate models with similar predictive power (e.g. F1 metric), the simplest model is most likely to be the best choice according to Occam's razor.\n",
    "\n",
    "To avoid overfitting on the test data, we perform cross-validation on the train data. As we don't want to overfit on a specific user's data, we use LeaveOneGroupOut cross-validation. Specifically, this means that we train a model several times, on all users but one, and we validate the model performance on that user. This is repeated for all the users in the train set, and gives us a robust estimate of the model performance.\n",
    "\n",
    "Note that some models require that the data is standardized to ensure model convergence. As such, after the unimportant features have been removed, we standarize each of the individual features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ijQy-bxL6RFB",
    "outputId": "50347fb0-979c-4142-d795-6023b512ec63"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nearest Neighbors\n",
      "0.7908325221375537\n",
      "Linear SVM\n",
      "0.8126888579986072\n",
      "Decision Tree\n",
      "0.6951820105653463\n",
      "Random Forest\n",
      "0.7982419825904432\n",
      "Neural Net\n",
      "0.818158676797512\n",
      "AdaBoost\n",
      "0.2105508095798306\n",
      "Naive Bayes\n",
      "0.6485754712694922\n",
      "LogisticRegression\n",
      "0.8026241120054891\n"
     ]
    }
   ],
   "source": [
    "names = [\"Nearest Neighbors\", \"Linear SVM\",\n",
    "         \"Decision Tree\", \"Random Forest\", \"Neural Net\", \"AdaBoost\",\n",
    "         \"Naive Bayes\", \"LogisticRegression\"]\n",
    "\n",
    "classifiers = [\n",
    "    KNeighborsClassifier(),\n",
    "    SVC(kernel=\"linear\"),\n",
    "    DecisionTreeClassifier(),\n",
    "    RandomForestClassifier(),\n",
    "    MLPClassifier(),\n",
    "    AdaBoostClassifier(),\n",
    "    GaussianNB(),\n",
    "    LogisticRegression()]\n",
    "performance = {}\n",
    "\n",
    "X_train_selected = X_train.drop(boruta_unimportant_features+mi_unimportant_features, axis=1)\n",
    "sc = StandardScaler()\n",
    "X_train_std = sc.fit_transform(X_train_selected)\n",
    "\n",
    "for name, clf in zip(names, classifiers):\n",
    "    print(name)\n",
    "    scores = cross_validate(clf, X=X_train_std, groups=subject_train.subjects.values, y=y_train, cv=GroupKFold(5), n_jobs=4, scoring='f1_macro')\n",
    "    performance[clf] = scores\n",
    "    print(np.mean(scores['test_score']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sor_tJdg6RFF"
   },
   "source": [
    "##  Hyperparameter tuning\n",
    "\n",
    "Hyperparameter tuning is the task of choosing a set of optimal hyperparameters for a machine learning algorithm or model. A hyperparameter, generally, is a parameter whose value is used to control the learning process. We'll investigate whether hyperparameter tuning has a significant effect in terms of performance for the Linear SVM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zuXardXV6RFF",
    "outputId": "adfb4e13-98f7-4ce5-fc78-38e84320aef4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 40 candidates, totalling 200 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  10 tasks      | elapsed:    3.7s\n",
      "[Parallel(n_jobs=-1)]: Done  64 tasks      | elapsed:   43.2s\n",
      "[Parallel(n_jobs=-1)]: Done 154 tasks      | elapsed:  4.0min\n",
      "[Parallel(n_jobs=-1)]: Done 200 out of 200 | elapsed:  5.3min finished\n",
      "/opt/anaconda3/envs/har_env/lib/python3.8/site-packages/sklearn/svm/_base.py:946: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=GroupKFold(n_splits=5), error_score=nan,\n",
       "             estimator=LinearSVC(C=1.0, class_weight=None, dual=True,\n",
       "                                 fit_intercept=True, intercept_scaling=1,\n",
       "                                 loss='squared_hinge', max_iter=1000,\n",
       "                                 multi_class='ovr', penalty='l2',\n",
       "                                 random_state=None, tol=0.0001, verbose=0),\n",
       "             iid='deprecated', n_jobs=-1,\n",
       "             param_grid={'C': [0.0001, 0.001, 0.01, 0.1, 1],\n",
       "                         'class_weight': ['balanced', None],\n",
       "                         'loss': ['hinge', 'squared_hinge'],\n",
       "                         'penalty': ['l1', 'l2']},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring='f1_macro', verbose=5)"
      ]
     },
     "execution_count": 40,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "penalty = ['l1', 'l2']\n",
    "loss = ['hinge', 'squared_hinge']\n",
    "C = [0.0001,0.001, 0.01,0.1,1]\n",
    "class_weight = ['balanced', None]\n",
    "# Create the random grid\n",
    "grid = {'penalty': penalty,\n",
    "               'loss': loss,\n",
    "               'C': C,\n",
    "               'class_weight': class_weight}\n",
    "\n",
    "# Use the random grid to search for best hyperparameters\n",
    "# First create the base model to tune\n",
    "svc = LinearSVC()\n",
    "# Random search of parameters, using 3 fold cross validation, \n",
    "# search across 100 different combinations, and use all available cores\n",
    "svc_grid = GridSearchCV(estimator = svc, scoring='f1_macro', param_grid = grid, cv = GroupKFold(5), verbose=5, n_jobs = -1)\n",
    "# Fit the random search model\n",
    "svc_grid.fit(X_train_std, y_train.values.ravel(), groups=subject_train.subjects.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HowktsTa6RFJ"
   },
   "source": [
    "We plot the model performance across the parameter grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dEpKErPV6RFJ",
    "outputId": "fc663459-5534-45fc-e17d-796e6846f0f7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'whiskers': [<matplotlib.lines.Line2D at 0x7fecf7c3e8e0>,\n",
       "  <matplotlib.lines.Line2D at 0x7fecf7c3ec40>],\n",
       " 'caps': [<matplotlib.lines.Line2D at 0x7fecf7c3efa0>,\n",
       "  <matplotlib.lines.Line2D at 0x7fecf7c15340>],\n",
       " 'boxes': [<matplotlib.lines.Line2D at 0x7fecf7c3e580>],\n",
       " 'medians': [<matplotlib.lines.Line2D at 0x7fecf7c156a0>],\n",
       " 'fliers': [<matplotlib.lines.Line2D at 0x7fecf7c15a00>],\n",
       " 'means': []}"
      ]
     },
     "execution_count": 41,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_scores = svc_grid.cv_results_['mean_test_score']\n",
    "test_scores = test_scores[~np.isnan(test_scores)]\n",
    "plt.boxplot(test_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t30cfWG-6RFM"
   },
   "source": [
    "We select the best model and train it on the full training data with the optimized features and hyper parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ja3-aDzv6RFM",
    "outputId": "9519dbec-5905-4a53-8805-eab07e263d88"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/har_env/lib/python3.8/site-packages/sklearn/svm/_base.py:946: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    }
   ],
   "source": [
    "sc = StandardScaler()\n",
    "X_train_selected = X_train.drop(boruta_unimportant_features+mi_unimportant_features, axis=1)\n",
    "X_train_std = sc.fit_transform(X_train_selected)\n",
    "\n",
    "clf = svc_grid.best_estimator_.fit(X_train_std, y_train.values.ravel())\n",
    "\n",
    "X_test_selected = X_test.drop(boruta_unimportant_features+mi_unimportant_features, axis=1)\n",
    "X_test_std = sc.transform(X_test_selected)\n",
    "\n",
    "y_pred_test = clf.predict(X_test_std)\n",
    "y_pred_train = clf.predict(X_train_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V6Eim3vs6RFO",
    "outputId": "af36444e-4572-4e0a-8787-6a50c5e7744e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9663999359273435, 0.8586635973551529)"
      ]
     },
     "execution_count": 47,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_pred_train, y_train, average='macro'), f1_score(y_pred_test, y_test, average='macro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rutA_MtW6RFQ"
   },
   "source": [
    "## Summary\n",
    "\n",
    "The optimised SVC model gives us similar performance, but we are using less features and are overfitting less! This is especially important if we would like to implement a model that runs in real-time. In the real-time context, we often have to trade off speed for performance.\n",
    "\n",
    "As this is still a linear model, training is fast, but we don't observe significant changes in performance. In the next notebook, we'll look at more complex models and optimization techniques."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "5. Advanced Modeling.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
