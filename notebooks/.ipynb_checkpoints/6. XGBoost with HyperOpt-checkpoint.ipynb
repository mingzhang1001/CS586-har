{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "ADJoM3Mr2U07"
   },
   "source": [
    "# XGBoost with HyperOpt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LDtQsBL_2U08"
   },
   "source": [
    "State of the art in machine learning modeling across a range of domains is the use of XGBoost models with hyperparameter search through a Bayesian algorithm.\n",
    "\n",
    "XGBoost is an optimized distributed gradient boosting library designed to be highly efficient. It implements machine learning algorithms under the Gradient Boosting framework.\n",
    "\n",
    "Hyperopt is a Python library for serial and parallel optimization over awkward search spaces such as hyper-parameter spaces, which may include real-valued, discrete, and conditional dimensions. It uses Parzen-Tree based search, which often outperforms random or grid search in terms of required time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G6XrfToY2U09"
   },
   "source": [
    "## Data Loading\n",
    "Import the required libraries to build an optimized XGBoost model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Tip5Um-E2U0-"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement hyperpot (from versions: none)\n",
      "ERROR: No matching distribution found for hyperpot\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'hyperopt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-44837c1f3bb4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mLeaveOneGroupOut\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mGroupKFold\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcross_validate\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mxgboost\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mXGBClassifier\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mhyperopt\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mTrials\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSTATUS_OK\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtpe\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfmin\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mspace_eval\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mxgboost\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mxgb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mplot_confusion_matrix\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf1_score\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclassification_report\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mroc_curve\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mroc_auc_score\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mauc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'hyperopt'"
     ]
    }
   ],
   "source": [
    "!pip install hyperpot\n",
    "import sys\n",
    "import math\n",
    "import json\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "sys.path.insert(1, '../src/')\n",
    "from utils import load_dataset_data\n",
    "from sklearn.model_selection import LeaveOneGroupOut, GroupKFold, cross_validate\n",
    "from xgboost import XGBClassifier\n",
    "from hyperopt import Trials, STATUS_OK, tpe, hp, fmin, space_eval\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import plot_confusion_matrix, f1_score, confusion_matrix, accuracy_score, classification_report, roc_curve, roc_auc_score,auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZMSrkAvs2U1C"
   },
   "source": [
    "We load the data and remove the unimportant features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hz5KUdPk2U1D"
   },
   "outputs": [],
   "source": [
    "X_train, y_train, subject_train, X_test, y_test, subject_test = load_dataset_data()\n",
    "\n",
    "with open('unimportant_features.json', 'r') as json_file:\n",
    "    unimportant_features = json.load(json_file)\n",
    "boruta_unimportant_features = unimportant_features['boruta']\n",
    "mi_unimportant_features = unimportant_features['mi_unimportant_features']\n",
    "\n",
    "X_train = X_train.drop(boruta_unimportant_features+mi_unimportant_features, axis=1)\n",
    "X_test = X_test.drop(boruta_unimportant_features+mi_unimportant_features, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7R7WqmB82U1I"
   },
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "54FtYUM02U1J"
   },
   "source": [
    "We'll first define the optimization objective, which is the F1 macro score on the CV groups. We also define the hyperparameter space, unique to the XGBoost model, that we want to optimize on. As we are using a more advanced search technique, we can make the search space larger. Hyperopt will intelligently search this hyperparameter space, rather than sampling randomly, or running the entire grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dDyqfaKp2U1J",
    "outputId": "3bb4b4bc-4964-4d90-c0c0-7b9d905621a5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  6%|â–‹         | 1/16 [17:06<4:16:38, 1026.59s/trial, best loss: 0.16335054900067758]"
     ]
    }
   ],
   "source": [
    "def objective(space):\n",
    "    clf = xgb.XGBClassifier(n_estimators = space['n_estimators'],\n",
    "                            max_depth = int(space['max_depth']),\n",
    "                            learning_rate = space['learning_rate'],\n",
    "                            gamma = space['alpha'],\n",
    "                            min_child_weight = space['min_child_weight'],\n",
    "                            subsample = space['subsample'],\n",
    "                            colsample_bytree = space['colsample_bytree'],\n",
    "                            colsample_bylevel = space['colsample_bylevel']\n",
    "                            )\n",
    "    \n",
    "    # Applying Group k-Fold Cross Validation\n",
    "    scores = cross_validate(clf, X=X_train, groups=subject_train.subjects.values, y=y_train.values.ravel(), cv=GroupKFold(10), n_jobs=4, scoring='f1_macro')\n",
    "    return{'loss':1-np.mean(scores['test_score']), 'status': STATUS_OK }\n",
    "\n",
    "space = {\n",
    "    'max_depth' : hp.choice('max_depth', range(2, 5, 1)),\n",
    "    'learning_rate' : hp.quniform('learning_rate', 0.2, 0.3, 0.01),\n",
    "    'n_estimators' : hp.choice('n_estimators', range(512, 1024, 64)),\n",
    "    'alpha' : hp.quniform('alpha', 1, 1.5, 0.05),\n",
    "    'min_child_weight' : hp.quniform('min_child_weight', 5, 10, 0.5),\n",
    "    'subsample' : hp.quniform('subsample', 0.75, 1, 0.05),\n",
    "    'colsample_bytree' : hp.quniform('colsample_bytree', 0.1, 0.5, 0.05),\n",
    "    'colsample_bylevel' : hp.quniform('colsample_bylevel', 0.5, 1, 0.05)\n",
    "}\n",
    "\n",
    "trials = Trials()\n",
    "best = fmin(fn=objective,\n",
    "            space=space,\n",
    "            algo=tpe.suggest,\n",
    "            max_evals=16,\n",
    "            trials=trials)\n",
    "\n",
    "print(\"Best: \", best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RVRtd_Nk2U1O"
   },
   "source": [
    "Once we've found the optimal parameters, we train the final model with the full training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LglM1p2j2U1P",
    "outputId": "ef590b07-1a56-4db2-9f85-2df291a53101"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=0.5, gamma=0.1, gpu_id=-1,\n",
       "              importance_type='gain', interaction_constraints='',\n",
       "              learning_rate=0.1, max_delta_step=0, max_depth=4,\n",
       "              min_child_weight=3.0, missing=nan, monotone_constraints='()',\n",
       "              n_estimators=116, n_jobs=0, num_parallel_tree=1,\n",
       "              objective='multi:softprob', random_state=0, reg_alpha=0,\n",
       "              reg_lambda=1, scale_pos_weight=None, subsample=0.7000000000000001,\n",
       "              tree_method='exact', validate_parameters=1, verbosity=None)"
      ]
     },
     "execution_count": 35,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_params = space_eval(space, best)\n",
    "clf = xgb.XGBClassifier(n_estimators = space['n_estimators'],\n",
    "                            max_depth = int(space['max_depth']),\n",
    "                            learning_rate = space['learning_rate'],\n",
    "                            gamma = space['alpha'],\n",
    "                            min_child_weight = space['min_child_weight'],\n",
    "                            subsample = space['subsample'],\n",
    "                            colsample_bytree = space['colsample_bytree'],\n",
    "                            colsample_bylevel = space['colsample_bylevel']\n",
    "                            )\n",
    "\n",
    "clf.fit(X_train, y_train.values.ravel())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qhwjQtih2U1T"
   },
   "source": [
    "We evaluate the test set on the optimized XGB model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g-I6nGGU2U1U",
    "outputId": "b8a80c3d-23e8-40e7-e0d6-6fe769cb7232"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9973489950215976, 0.8454214967936583)"
      ]
     },
     "execution_count": 36,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_test = clf.predict(X_test)\n",
    "y_pred_train = clf.predict(X_train)\n",
    "f1_score(y_pred_train, y_train, average='macro'), f1_score(y_pred_test, y_test, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HC5qOb5Y2U1f",
    "outputId": "56c61b21-3ed8-4c6d-8884-68d984f38fdb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9997425003218746, 0.92662871600253)"
      ]
     },
     "execution_count": 39,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_pred_train, y_train), accuracy_score(y_pred_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BphN777h2U1i"
   },
   "source": [
    "## Summary\n",
    "\n",
    "The current set of trials were unable to find a significantly better model. For this reason, we'll need a larger number of trials and a larger parameter grid.\n",
    "\n",
    "Running a Hyperopt job requires significant amounts of computation resources and a large number of evaluation rounds are required to achieve optimal performance. For this reason, we'll use a cloud resource, AWS Sagemaker to perform the optimization. Sagemaker is able to scale out our search for the optimal algorithm."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "6. XGBoost with HyperOpt.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
