{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# AWS Sagemaker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Amazon SageMaker is a managed service that provides data scientists with the ability to build, train, and deploy machine learning (ML) models quickly. \n",
    "\n",
    "We'll be using Amazon SageMaker automatic model tuning, also known as hyperparameter tuning. This service finds the best version of a model by running many training jobs on the HAR dataset using the algorithm and ranges of hyperparameters that are specified. It then chooses the hyperparameter values that result in a model that performs the best, as measured by our metric (F1 score)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading\n",
    "Import the required libraries to build the Sagemaker model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'xgboost'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-7055dbc395ea>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mutils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mload_dataset_data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mLeaveOneGroupOut\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mGroupKFold\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mGroupShuffleSplit\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mxgboost\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mXGBClassifier\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msagemaker\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mamazon\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mamazon_estimator\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mget_image_uri\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtime\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mgmtime\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstrftime\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msleep\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'xgboost'"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import math\n",
    "import json\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from time import gmtime, strftime\n",
    "%matplotlib inline\n",
    "sys.path.insert(1, '../src/')\n",
    "from utils import load_dataset_data\n",
    "from sklearn.model_selection import LeaveOneGroupOut, GroupKFold, GroupShuffleSplit\n",
    "from xgboost import XGBClassifier\n",
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "from time import gmtime, strftime, sleep\n",
    "from sklearn.metrics import plot_confusion_matrix, f1_score, confusion_matrix, accuracy_score, classification_report, roc_curve, roc_auc_score,auc\n",
    "import sagemaker\n",
    "import boto3\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup a connection to AWS Sagemaker and S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = boto3.Session(aws_access_key_id= 'AKIAZIIW3ED4EWTNGZHP', aws_secret_access_key='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', region_name='us-east-1')\n",
    "client = session.client('sagemaker')\n",
    "role = 'arn:aws:iam::636239945976:role/service-role/AmazonSageMaker-ExecutionRole-20201011T120068'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_session = sagemaker.Session(boto_session=session)\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "prefix = 'hardataset1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the data and split a static train and validation dataset as Sagemaker does not support cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, subject_train, X_test, y_test, subject_test = load_dataset_data()\n",
    "\n",
    "with open('unimportant_features.json', 'r') as json_file:\n",
    "    unimportant_features = json.load(json_file)\n",
    "boruta_unimportant_features = unimportant_features['boruta']\n",
    "mi_unimportant_features = unimportant_features['mi_unimportant_features']\n",
    "\n",
    "X_train = X_train.drop(boruta_unimportant_features+mi_unimportant_features, axis=1)\n",
    "X_test = X_test.drop(boruta_unimportant_features+mi_unimportant_features, axis=1)\n",
    "\n",
    "kfolds = GroupKFold(5)\n",
    "train, val = next(kfolds.split(X_train, y_train, subject_train))\n",
    "\n",
    "df_train = pd.concat([y_train.iloc[train], X_train.iloc[train]], axis=1)\n",
    "df_val = pd.concat([y_train.iloc[val], X_train.iloc[val]], axis=1)\n",
    "\n",
    "df_train.activity_label = (df_train.activity_label-1)\n",
    "df_val.activity_label = (df_val.activity_label-1)\n",
    "\n",
    "df_train.to_csv('train.csv', ',', index=False, header=False)\n",
    "df_val.to_csv('validation.csv', ',', index=False, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload the train and validation data to S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.resource('s3').Bucket(bucket).Object(os.path.join(prefix, 'train_har/train.csv')).upload_file('train.csv')\n",
    "session.resource('s3').Bucket(bucket).Object(os.path.join(prefix, 'validation_har/validation.csv')).upload_file('validation.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll first define the the hyperparameter space, unique to the XGBoost model, that we want to optimize on. As we are using a more advanced search technique (Bayesian), we can make the search space larger. Sagemaker will intelligently search this hyperparameter space, rather than sampling randomly, or running the entire grid.\n",
    "We'll optimize towards 'validation:merror' as 'validation:f1' is not yet supported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xgboost-tuningjob-13-11-34-36\n"
     ]
    }
   ],
   "source": [
    "tuning_job_name = 'xgboost-tuningjob-' + strftime(\"%d-%H-%M-%S\", gmtime())\n",
    "\n",
    "tuning_job_config = {\n",
    "    \"ParameterRanges\": {\n",
    "      \"CategoricalParameterRanges\": [],\n",
    "      \"ContinuousParameterRanges\": [\n",
    "        {\n",
    "          \"MaxValue\": \"1\",\n",
    "          \"MinValue\": \"0\",\n",
    "          \"Name\": \"eta\",\n",
    "        },\n",
    "        {\n",
    "          \"MaxValue\": \"15\",\n",
    "          \"MinValue\": \"5\",\n",
    "          \"Name\": \"min_child_weight\",\n",
    "        },\n",
    "        {\n",
    "          \"MaxValue\": \"2\",\n",
    "          \"MinValue\": \"0\",\n",
    "          \"Name\": \"alpha\",            \n",
    "        },\n",
    "        {\n",
    "          \"MaxValue\": \"1\",\n",
    "          \"MinValue\": \"0\",\n",
    "          \"Name\": \"subsample\",            \n",
    "        },\n",
    "        {\n",
    "          \"MaxValue\": \"1\",\n",
    "          \"MinValue\": \"0\",\n",
    "          \"Name\": \"colsample_bytree\",            \n",
    "        },\n",
    "                    {\n",
    "          \"MaxValue\": \"1\",\n",
    "          \"MinValue\": \"0\",\n",
    "          \"Name\": \"colsample_bylevel\",            \n",
    "        }\n",
    "      ],\n",
    "      \"IntegerParameterRanges\": [\n",
    "        {\n",
    "          \"MaxValue\": \"6\",\n",
    "          \"MinValue\": \"1\",\n",
    "          \"Name\": \"max_depth\",\n",
    "        },\n",
    "        {\n",
    "          \"MaxValue\": \"1024\",\n",
    "          \"MinValue\": \"512\",\n",
    "          \"Name\": \"num_round\",\n",
    "        }, \n",
    "      ]\n",
    "    },\n",
    "    \"ResourceLimits\": {\n",
    "      \"MaxNumberOfTrainingJobs\": 256,\n",
    "      \"MaxParallelTrainingJobs\": 4\n",
    "    },\n",
    "    \"Strategy\": \"Bayesian\",\n",
    "    \"HyperParameterTuningJobObjective\": {\n",
    "      \"MetricName\": \"validation:merror\",\n",
    "      \"Type\": \"Minimize\"\n",
    "    },\n",
    "  }\n",
    "print (tuning_job_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define the training and validation config, and we define the Sagemaker resources that we'll be using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_image = get_image_uri('us-east-1', 'xgboost', repo_version='latest')\n",
    "     \n",
    "s3_input_train = 's3://{}/{}/train_har/'.format(bucket, prefix)\n",
    "s3_input_validation ='s3://{}/{}/validation_har/'.format(bucket, prefix)\n",
    "    \n",
    "training_job_definition = {\n",
    "    \"AlgorithmSpecification\": {\n",
    "      \"TrainingImage\": training_image,\n",
    "      \"TrainingInputMode\": \"File\"\n",
    "    },\n",
    "    \"InputDataConfig\": [\n",
    "      {\n",
    "        \"ChannelName\": \"train\",\n",
    "        \"CompressionType\": \"None\",\n",
    "        \"ContentType\": \"csv\",\n",
    "        \"DataSource\": {\n",
    "          \"S3DataSource\": {\n",
    "            \"S3DataDistributionType\": \"FullyReplicated\",\n",
    "            \"S3DataType\": \"S3Prefix\",\n",
    "            \"S3Uri\": s3_input_train\n",
    "          }\n",
    "        }\n",
    "      },\n",
    "      {\n",
    "        \"ChannelName\": \"validation\",\n",
    "        \"CompressionType\": \"None\",\n",
    "        \"ContentType\": \"csv\",\n",
    "        \"DataSource\": {\n",
    "          \"S3DataSource\": {\n",
    "            \"S3DataDistributionType\": \"FullyReplicated\",\n",
    "            \"S3DataType\": \"S3Prefix\",\n",
    "            \"S3Uri\": s3_input_validation\n",
    "          }\n",
    "        }\n",
    "      }\n",
    "    ],\n",
    "    \"OutputDataConfig\": {\n",
    "      \"S3OutputPath\": \"s3://{}/{}/output_har\".format(bucket,prefix)\n",
    "    },\n",
    "    \"ResourceConfig\": {\n",
    "      \"InstanceCount\": 1,\n",
    "      \"InstanceType\": \"ml.m4.xlarge\",\n",
    "      \"VolumeSizeInGB\": 10\n",
    "    },\n",
    "    \"RoleArn\": role,\n",
    "    \"StaticHyperParameters\": {\n",
    "      \"objective\": \"multi:softmax\",\n",
    "      \"num_class\": \"12\"\n",
    "    },\n",
    "    \"StoppingCondition\": {\n",
    "      \"MaxRuntimeInSeconds\": 43200\n",
    "        \n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the hyper parameter tuning job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'HyperParameterTuningJobArn': 'arn:aws:sagemaker:us-east-1:636239945976:hyper-parameter-tuning-job/xgboost-tuningjob-13-11-34-36',\n",
       " 'ResponseMetadata': {'RequestId': '0ba591c9-b014-4b77-b0a5-4193661c981d',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '0ba591c9-b014-4b77-b0a5-4193661c981d',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '130',\n",
       "   'date': 'Tue, 13 Oct 2020 11:34:50 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.create_hyper_parameter_tuning_job(HyperParameterTuningJobName = tuning_job_name,\n",
    "                                            HyperParameterTuningJobConfig = tuning_job_config,\n",
    "                                            TrainingJobDefinition = training_job_definition)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We verify whether the hyper parameter tuning job is running and list the different jobs. The jobs are sorted by objective metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tuning_job_name = \"xgboost-tuningjob-13-06-58-23\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_jobs = client.list_training_jobs_for_hyper_parameter_tuning_job(\n",
    "    HyperParameterTuningJobName=tuning_job_name, SortBy=\"FinalObjectiveMetricValue\", SortOrder='Ascending')[\"TrainingJobSummaries\"]\n",
    "job_list = []\n",
    "for job in training_jobs:\n",
    "    if 'FinalHyperParameterTuningJobObjectiveMetric' in job:\n",
    "        value = job[\"FinalHyperParameterTuningJobObjectiveMetric\"]['Value']\n",
    "        params = job['TunedHyperParameters']\n",
    "        job_list.append((params, value))\n",
    "job_list.sort(key=lambda x: x[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select the best job as the model that we want to replicate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_job = job_list[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train the model with the best set of hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'alpha': 1.7632819381608957,\n",
       " 'colsample_bylevel': 0.2719193271114313,\n",
       " 'colsample_bytree': 0.465505553275144,\n",
       " 'eta': 0.03384446477878989,\n",
       " 'max_depth': 6,\n",
       " 'min_child_weight': 12.281818465934075,\n",
       " 'subsample': 0.8434874449635632,\n",
       " 'n_estimators': 638}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = {}\n",
    "for k,v in best_job.items():\n",
    "    if k != 'num_round':\n",
    "        params[k] = float(v)\n",
    "params['n_estimators'] = int(best_job['num_round'])\n",
    "params['max_depth'] = int(best_job['max_depth'])\n",
    "clf = XGBClassifier()\n",
    "clf.set_params(**params)\n",
    "clf.fit(X_train, y_train.values.ravel())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We evaluate the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9922014508130036, 0.8484351929650312)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_test = clf.predict(X_test)\n",
    "y_pred_train = clf.predict(X_train)\n",
    "f1_score(y_pred_train, y_train.values, average='macro'), f1_score(y_pred_test, y_test, average='macro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "The current set of trials were able to find a better model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
